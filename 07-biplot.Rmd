# Exploring compositional data: the compositional biplot {#biplot}

\hspace{2cm}\begin{minipage}[ct]{10cm}
\parskip=5pt
\parindent=5pt
This example was first presented as part of the CoDa microbiome tutorial in Barcelona, Spain in April 2018 at the NGS'18 conference.  It has been modified for clarity and completeness for this book.
\end{minipage}
\vspace{1cm}


When analyzing and interpreting compositional data, it is important to remember that we are examining the variance in the ratios of the underlying data, and not directly examining abundance. The first tool that we will use is the compositional biplot. This is generated by the following set of steps:

1. remove essential 0 values (0s that are in all samples. i.e. nondetects)
2. perform any additional filtering (sparsity, minimal abundance, minimum sample count, etc)
3. adjust remaining 0 values with the zCompositions package
4. perform the clr transform on the data
5. conduct a singular value decomposition using prcomp
6. display the results in a principle component plot

Let us see how this works in principle. We will make a sample dataset that has 30 samples and only nine features. Samples will be in two groups. The first group of 20 (1-20) will differ from the last group of 10 (21-30). Feature A will be more abundant in the first 20 samples, and less abundant in the last 10 , features B, C and D will be the opposite. Features C and D will be simple transforms of feature B such that feature C has about a 5000-fold greater difference between groups than feature B, but the same relative variance. Feature D will have the square of the variance of feature B. The remaining features will be highly variable, but at random. For simplicity we will use a random-normal distribution, but any other distribution would work as well.


```{r biplot_data}
```

## The compositional biplot

The compositional biplot in Figure \ref{fig:biplot} shows both the features (variables A-I) and the distances between samples on one PCA (principle component) plot.

The features are represented by the red arrows and the red letters. The origin of the arrows is the midpoint of the data; in this case the geometric mean of the dataset since we are using the centred log-ratio tranform. The length of the arrow is proportional to the standard deviation of the feature, up to the resolution of the PCA plot. The arrow points in the direction of greater relative abundance in the dataset for the feature. The resolution of the plot is the proportion of the variance explained by the principle components plotted, here the proportion explained is 0.775. With this amount of variance we have a fairly good representation of the dataset.

By design, the standard deviation of each feature is one quarter the mean value, and so the length of all arrows should be exactly the same. Note that this is obviously not true; and this shows the limitation of such a representation. The best we can hope for  is that the axis of the experiment is on one of the major components as it his here. This dataset has 9 features, and so is actually represented by an 8-dimensional simplex. The PCA plot is a rotation of that simplex such that the maximum variance possible is displayed in the first two dimensions.

While a compositional biplot can contain much information [@Ait1983; @aitchison2002biplots], in the context of a microbiome or transcriptome dataset that contains hundreds or thousands of features, the complexity can rapidly become overwhelming. Thus, I will only indicate the major observations.

1. The positions of the samples (1-30) are represented by a projection of their distance relationship on the simplex, up to the limit of the variance explained. In this case, we can see that samples 1-20 and 21-30 separate into to groups, with the possible exception of sample 27.

 2. The arrows (or rays) for each feature are different lengths because they are a projection (shadow) of an 8-dimensional space onto two dimensions. Thus, the obviously shorter arrows (E, H, B, C) are projecting into other dimensions; for the sake of simplicity, assume they are projecting above or below the plane of the Figure \ref{fig:biplot}. The total length of each arrow (the variance of each feature) will be the same in this example if all dimensions were measured.

3. The angle between the arrows for each feature is indicative of their Pearson's correlation, just as it is in a traditional biplot, with a smaller angle implying a greater correlation. By this measure correlated sets of features could include features G and I, or could include features B,C and D. However, correlation in compositional data is not enough because we are measuring the ratios between features, not their absolute values [@aitchison2002biplots; @Lovell:2015; Erb134536].

4. Features represented by any two arrows that are not nearly co-incident indicate pairs of features that are likely uncorrelated. Note that in a compositional biplot, features represented by arrows pointing in opposite directions \emph{may or may not} indicate anti-correlated features as shown in Figure \ref{fig:scatters} for the A:C pair, which is negatively correlated by design, and the G:F pair which appears to be negatively correlated but is actually not correlated at all. It is dangerous to infer negative correlation in compositional data because there are many apparent sources.

5. Any two arrows with equal lengths (two features with the same relative standard deviation) will have a constant ratio relationship; that is, $F_1 = F_2 * C$, where the two features are related by a common multiplicative constant $C$. We can see this clearly for the B,C pair, which have the same length but differ by a factor of about 5000 in absolute abundance while keeping their relative standard deviation the same.

6. Any two arrows with dissimilar lengths that have a small angle between them but a different length are related by an exponential relationship, and will be correlated but will not be in a constant ratio as shown in Figure \ref{fig:scatters}.




```{r biplot, echo=FALSE, results='show', fig.width=8, fig.height=8, message=FALSE,fig.cap="Compositional biplot of the features A-I.  From this we can see that the absolute abundance is not represented in the biplot."}

# extract the principle components and loadings
a_i.pcx <- prcomp(a_i.clr)

# make a compositional biplot
biplot(a_i.pcx, var.axes=TRUE, scale=0,
    xlab=paste("PC1", round(a_i.pcx$sdev[1]^2 / sum(a_i.pcx$sdev^2),3), sep=": "),
    ylab=paste("PC2", round(a_i.pcx$sdev[2]^2 / sum(a_i.pcx$sdev^2),3), sep=": ")
) # covariance

```

Figure \ref{fig:scatters} shows the relationships between pairs of interest in the compositional biplot. The top left plot shows that features A and B are indeed negatively correlated in the dataset as designed. The top right plot shows that it is dangerous to infer negative correlation in compositional data as the F:G pair also appears to be negatively correlated but in fact are not correlated at all.


When examining correlation in these datasets, we are most interested in finding those features that have a near constant variance ratio. In the context of a scatter plot of clr values this shows as pairs where the correlation is strong, and the slope of the correlation is near unity. The bottom left panel shows the plof of the B:C pair, which has a correlation of 0.093, and a slope of best fit of 0.92, close to 1. In contrast, the B:D pair has an even higher correlation of 0.99, but the slope of best fit is 0.58, and is clearly not close to 1. Thus, in this dataset only the B:C pair is likely to be compositionally associated.

```{r scatters, echo=FALSE,, results='show', fig.width=8, fig.height=8, message=FALSE,fig.cap="Counts were generated from a random normal distribution for 9 features labeled A-I. Thirty samples were generated, and  features A,B,C and D were differentially abundant in the first twenty and last 10 samples. The others were of widely different abundances, but with random change between samples. Feature C is a randomly perturbed feature B, and feature D is a feature B powered by a random range between 1.4-1.5. Feature B is plotted vs features C and D as both counts and as centre log-ratio transformed values."}

par(mfrow=c(2,2))

plot(a_i.clr[,"A"]~ a_i.clr[,"B"], xlab="B", ylab="A", main="cor: -0.88")

plot(a_i.clr[,"G"]~ a_i.clr[,"F"], xlab="F", ylab="G", main="cor: -0.16")


plot(a_i.clr[,"B"]~ a_i.clr[,"C"], xlab="C", ylab="B", main="cor: 0.93")
abline(-8.02, 0.92, lty=2, col="blue")
abline(-8.5, 1, lty=2, col="red")


plot(a_i.clr[,"B"]~ a_i.clr[,"D"], xlab="D", ylab="B", main="cor: 0.99")
abline(-1.72, 0.586, lty=2, col="blue")
abline(-1.253, 1, lty=2, col="red")

```




```{r, echo=FALSE, results='show', fig.width=8, fig.height=8, warning=FALSE, message=FALSE,fig.cap="The advantage of compositions is that the results are largely invariant to subsetting. That is, we get essentially the same answer with all (A-H), and with a subset of the data (A-G). We also see that in these data, the variance of the clr transformed values is much more informative than the absolute, or proportional values, and better represents the known structure of the data. "}

a_i <- cbind(A,B,C,D,E,F,G,H)
a_i.prop <- t(apply(a_i, 1, function(x){x/sum(x)}))
a_i.clr <- t(apply(a_i.prop, 1, function(x){log(x) - mean(log(x))}))
a_i.pcx <- prcomp(a_i.clr)
a_i.p.pcx <- prcomp(a_i.prop)
a_i.c.pcx <- prcomp(a_i)

a_g <- cbind(A,B,C,D,E,F,G)
a_g.prop <- t(apply(a_g, 1, function(x){x/sum(x)}))
a_g.clr <- t(apply(a_g.prop, 1, function(x){log(x) - mean(log(x))}))
a_g.pcx <- prcomp(a_g.clr)
a_g.p.pcx <- prcomp(a_g.prop)
a_g.c.pcx <- prcomp(a_g)

a_i.z <- apply(a_i, 2, function(x){ (x-mean(x))/sqrt(var(x)) })
a_i.z.prop <- apply(a_i.prop, 2, function(x){ (x-mean(x))/sqrt(var(x)) })
a_i.z.prop.pcx <- prcomp(a_i.z.prop)
a_i.z.pcx <- prcomp(a_i.z)

a_g.z <- apply(a_g, 2, function(x){ (x-mean(x))/sqrt(var(x)) })
a_g.z.prop <- apply(a_g.prop, 2, function(x){ (x-mean(x))/sqrt(var(x)) })
a_g.z.prop.pcx <- prcomp(a_g.z.prop)
a_g.z.pcx <- prcomp(a_g.z)

par(mfrow=c(2,3))
#biplot(a_i.pcx, var.axes=TRUE, scale=0) # form
biplot(a_i.pcx, var.axes=TRUE, scale=0,
    xlab=paste("PC1", round(a_i.pcx$sdev[1]^2 / sum(a_i.pcx$sdev^2),3), sep=": "),
    ylab=paste("PC2", round(a_i.pcx$sdev[2]^2 / sum(a_i.pcx$sdev^2),3), sep=": ")
    , main="A-H clr"
) # covariance

biplot(a_i.p.pcx, var.axes=TRUE, scale=0,
    xlab=paste("PC1", round(a_i.p.pcx$sdev[1]^2 / sum(a_i.p.pcx$sdev^2),3), sep=": "),
    ylab=paste("PC2", round(a_i.p.pcx$sdev[2]^2 / sum(a_i.p.pcx$sdev^2),3), sep=": ")
    , main="A-H prop"
) # covariance

biplot(a_i.c.pcx, var.axes=TRUE, scale=0,
    xlab=paste("PC1", round(a_i.c.pcx$sdev[1]^2 / sum(a_i.c.pcx$sdev^2),3), sep=": "),
    ylab=paste("PC2", round(a_i.c.pcx$sdev[2]^2 / sum(a_i.c.pcx$sdev^2),3), sep=": ")
    , main="A-H count"
) # covariance

biplot(a_g.pcx, var.axes=TRUE, scale=0,
    xlab=paste("PC1", round(a_g.pcx$sdev[1]^2 / sum(a_g.pcx$sdev^2),3), sep=": "),
    ylab=paste("PC2", round(a_g.pcx$sdev[2]^2 / sum(a_g.pcx$sdev^2),3), sep=": ")
    , main="A-G clr"
) # covariance

biplot(a_g.p.pcx, var.axes=TRUE, scale=0,
    xlab=paste("PC1", round(a_g.p.pcx$sdev[1]^2 / sum(a_g.p.pcx$sdev^2),3), sep=": "),
    ylab=paste("PC2", round(a_g.p.pcx$sdev[2]^2 / sum(a_g.p.pcx$sdev^2),3), sep=": ")
    , main="A-G prop"
) # covariance

biplot(a_g.c.pcx, var.axes=TRUE, scale=0,
    xlab=paste("PC1", round(a_g.c.pcx$sdev[1]^2 / sum(a_g.c.pcx$sdev^2),3), sep=": "),
    ylab=paste("PC2", round(a_g.c.pcx$sdev[2]^2 / sum(a_g.c.pcx$sdev^2),3), sep=": ")
    , main="A-G count"
) # covariance
```
