# Data transforms in high throughput sequencing {#transforms}

This chapter introduces data transformations and distance (or dissimilarity) metrics that are prevalent in the ecological literature, and that have been extensively used in analyzing high throughput sequencing datasets. It is not intended to be a comprehensive analysis of data transformations. In some cases, only one transformation is demonstrated when several transformations  are obviously related.

This chapter is rather information dense, but the lessons in it are very important to understand the limitations of data analysis whenever the data are compositional. I have tried to make it easy and intuitive, but in the end you must work through the examples as best you can. All the source code needed to explore the data are included either directly here, or in an external file when the amount of code would break the flow of the narrative.


## Why transform data?

Data are transformed for a variety of reasons. The first reason, is to make the data amenable to statistical assumptions for parametric tests that require the data be normally distributed with similar standard deviations in all groups. Can we transform the data to approximate a Gaussian distribution? This mode of thinking leads to the use of square-root, arcsine or Hellinger transformations since they appear to transform the data into a distribution that can be interpreted. However, as we shall see below, none of these univariate transformations  is suitable, and many of these historically common transformations have recently fallen out of favour, e.g., [@Warton:2011aa].

The logaritmic transformation is now commonly used after one of the  'count normalization' methods described below. As we shall see below, neither count normalized, nor log count normalized data is generally the best option. The second reason for normalization is to remove or adjust the compositional nature of the data [@Weiss:2017aa], again, as we shall see this is not possible. The assumption is that any conclusion made on the transformed data represents in some way changes in the underlying abundance of the input DNA molecules as outlined in Chapter \ref{sequencing}. As we shall see, this is not possible, but using a CoDa-based approach we can determine if changes in the \emph{relative values} of DNA molecules are altered; the question is "relative to what?".

Fundamentally, the goal of any experiment is to determine something about the environment that was sampled. After all, we are attempting to use HTS to determine something of interest about the underlying environment. Thus, we need to have some equivalence between the samples before sequencing and the samples after sequencing. The simplest case would be that there would be a linear relationship between the data that we could obtain from the environment, and the data that was actually collected by HTS.

## Sequencing changes the shape of all but the most ideal data:

We have seen that high throughput sequencing as currently practiced is constrained by the capacity of the instrument. Let us revisit the toy example in Figure \ref{shape}, where we had an unconstrained random sample of counts and see how well common data transformations work to recapitulate the overall shape of the data.


 the In this section I show that the practical consequence of the instrument constraint can be severe in all but the most idealized datasets. An idealized dataset is one where the total number of  molecules is the same per sampling effort in all the samples taken from the the environment. An example of such an idealized would be if the samples were taken from a cell culture experiment with a treatment and control group where the treatment was expected to alter only a small number of features. This would be an example of a constrained dataset: the total number of DNA molecules in the samples in the two groups would be expected to be substantially the same except for random variation. This assumption is implicit in all differential abundance tools in use.

An example of a less than ideal experiment would be comparing the total gene content in DNA isolated from two different ecosystems.

It is more desirable to think about HTS data in a multivariate way as a `composition'  because the total count of molecules in the underlying sample (the environment) is always a confounding variable [@Loven:2012aa]. This way of thinking led to multivariate data normalizations.


## Commonly used transformations are misleading

The `vegan` R package manual [@vegan:2017] has a very good description of many data transformations and I recommend it to the interested reader: note that the transforms in the `vegan` package are not compositionally appropriate, and I do not recommend them in general for the reasons outlined below.

Current practice is to examine the datasets using 'relative abundance' values, that is, the proportional abundance of the features either before or after normalization for read depth. This approach is equivalent to examining the input unconstrained data of the type seen in Figure \ref{fig:shape} in the relative abundance sample space in the bottom right panel of the figure after normalizing the total number of reads to be approximately constant. This approach will obviously lead to incorrect assumptions in at least some cases. For example, depending upon the steps chosen to compare, the blue feature, that has constant counts in the input, will be seen to either increase or decrease in abundance. Conversely, the black feature, that is always decreasing in abundance will be seen to be constant if comparing samples 1-8.


```{r transformations, echo=FALSE,fig.width=6,fig.height=4.5,warning=F,message=F, fig.cap="The effect of ecological transformations on unconstrained high throughput sequencing datasets. Data generated as in Figure \\ref{fig:shape} were normalized to a (near) constant total number of reads,  converted to proportions, then transformed with five different approaches implemented in the vegan ecological analysis package. The 'Counts' panel shows the original data, the other panels begin with a 'v:' and indicate the vegan package transformation. The transformation in the RLE panel is described below."}

# first make the total count similar for all datasets because sequencing is constrained
# a small random pseudocount is added so the log of 0 is never calculated
determ.m <- t(apply(m.dub.u, 1, function(x) ceiling(sum(m.dub.u[1,])/sum(x) * x) ))
determ.m.p <- t(apply(determ.m, 1, function(x) x + runif(100) /sum(x)))

freq.m <- t(apply(m.dub.u, 1, function(x) decostand(x, method="freq") ))
hel.m <- t(apply(determ.m.p, 1, function(x) sqrt(x)))
pa.m <- t(apply(determ.m.p, 1, function(x) 10^decostand(x, method="standardize")))
range.m <-  t(apply(determ.m.p, 1, function(x) decostand(x, method="range")))

log.m <- t(apply(determ.m.p , 1, function(x) 2^(decostand(x, method="log"))))
ran.dat.m <- t(des.norm(t(determ.m.p)))
ran.dat.c <- t(des.norm(t(m.dub.u)))
clr.m <- t(apply(determ.m.p, 1, function(x) log2(x) - mean(log2(x))))
par(mfrow=c(2,3), mar=c(4,4,3,1))

# unconstrained counts
plot_c(m.dub.u, main="Counts", ylab="log10 count", log="y")
plot_c(freq.m, main="v:Frequency", ylab="log10 value", log="y")
plot_c(hel.m, main="v:Hellinger", ylab="log10 value", log="y")
plot_c(log.m, main="v:Log", ylab="log10 value", log="y")
plot_c(ran.dat.m, main="RLE", ylab="log10 count", log="y")
plot_c(clr.m, main="CLR", ylab="log10 value")

```

The ecological literature offers many different transformations for such data, often as a way of making the data appear 'more normal'. Figure  \ref{fig:transformations} shows the results of a few such transformations that are in the `vegan R` package.

- The frequency transform divides the each feature value by the largest feature count, and then divides the resulting values by the number of features in the sample that had non-zero counts. This is the often referred to as 'relative abundance' and is a simple proportion.

- The Hellinger transformation that takes the square root of the relative abundance (proportion) value.

- The log transform divides each feature count in a sample by the minimum non-zero count value, then takes the logarithm of the resulting value and adds 1. Counts of 0 are assigned a value of 0 to avoid taking the logarithm of 0.

The RLE and CLR transforms are not instantiated in `vegan`, and are described below.

It is obvious that the Frequency, Hellinger, and Log transformations result in data that badly mis-represents the shape of the actual count data. All other transformations instantiated in the `vegan` package deliver data that is transformed even more extremely.  The log transformation would be suitable \emph{if} the total counts observed after sequencing was directly proportional to the total counts in the enviroment, however, we have seen that this condition cannot be met for high throughput sequencing. Thus, none of these transformations, though widely used, are suitable when analyzing high throughput sequencing data.

The RLE and CLR transforms appear to most closely recapture the shape of the original data and appear very similar. We will discuss these below.


Comparison of 'differential abundance' is problematic for compositional data [@fernandes:2013;@fernandes:2014]. Since the apparent abundance of every value depends on the apparent abundance of every other value, we can get into real difficulties if we are not careful. If we refer back to Figure \ref{fig:transformations}:Counts, and we  compare the relative abundances of features between sample 1 and sample 20, we  observe that only the black feature has changed in absolute abundance and that the count of all other features is unchanged. Note that we would be very wron in our inferences if we use the Frequency, Hellinger or Log transforms, since we would infer that the black feature increased, but that all other features had decreased in abundance. The RLE and CLR transformations do a better job of controlling for the interdependence between features. Both transforms have the black feature increasing, but the other features increase or decrease only marginally, and apparently at random.


## Other data transformations

I will introduce each of the transformations in turn, and then we will examine the effect of each transformation on the data. Let us see which, if any of the transforms fulfills the basic requirements set out above.

### Notation

We use the following notation throughout.

- Column vectors contain samples $\vec{\textbf{s}}$ and row vectors contain features $\vec{\textbf{f}}$
- There are $D$ features and $n$ samples, thus the data are contained in matrix of dimension $M = D \times n$
- The $j^{th}$ sample is denoted as $\vec{\textbf{s}}_{j}$
- The $i^{th}$ feature of all samples is denoted as $\vec{\textbf{f}}_{i-}$
- The value for the $i^{th}$ feature of the $j^{th}$ sample is referred to as $s_{ij}$
 We will consider the following transformations



### The proportional transformation

This simple normalization is to determine the relative abundance (rAB), or proportion, of the \ith{i} feature in a sample as in Eq. \ref{eq:rab}. This normalization is also referred to as the total sum scaling (TSS) normalization. The effect is shown in panel Frequency in Figure \ref{fig:transformations}.

\begin{equation}
	rAB_{i} = \frac{s_{i}}{\sum{\vec{\textbf{s}}}}
	\label{eq:rab}
\end{equation}

The rAB measure requires only the read count observed for the feature $s_{ij}$ and the total read count of the sample $\sum{\vec{\textbf{s}}}$. Since this measure is generally skewed, it is often log-transformed prior to analysis.

### The RPKM and TPM transformations

A further normalization was proposed early in the RNA-seq field where the reads per kilobase per million mapped (RPKM)[@Mortazavi:2008] method was used initially to place the read counts for each feature within and between samples on a common scale.

For this we also needed to know a scaling factor $K$, and the length of the feature $L_i$; from this, the RPKM value for the \ith{i} feature for each sample was calculated as in Eq. \ref{eq:rpkm}.

\begin{equation}
	RPKM_{i} = \frac{K \cdot s_{i} }{\sum{\vect{s}} \cdot L_{i}}
	\label{eq:rpkm}
\end{equation}

When the equation is placed in this form it is obvious that RPKM is simply a scaled rAB where each rAB value is divided by its length and multiplied by a constant. In compositional terms, RPKM is an unclosed perturbation of the original data; the data appear to be real numbers, but are actually proportions multiplied by a constant.

Further research suggested that RPKM was not appropriate for comparison of features between samples. The goal of RPKM was to `count' reads per feature per cell.  In the original paper the authors supplied an equivalence and an RPKM value of 1 RPKM equalled one transcript in each cell in the C2C12 cell line, but in liver cells, a value of 3 RPKM equalled one transcript per cell. Thus, from the start, this normalization was unable to normalize between-condition read counts.

The transcripts per million (TPM) normalization was advocated next [@Li:2010aa]. Patcher [@Pachter:2011] showed the equivalence between RPKM and TPM, and in compositional terms TPM is simply a compositionally closed form of RPKM multiple by a constant as in Eq. \ref{eq:tpm}.

\begin{equation}
	TPM_{i} = \frac{RPKM_i}{\sum{RPKM}} \cdot K
	\label{eq:tpm}
\end{equation}

The rAB, RPKM and TPM normalizations are thus all very similar, differing only in the scaling of individual features, and do not allow normalization between conditions unless the samples in the environment contain \emph{exactly} the same input number of RNA molecules. These normalizations deliver proportional data, scaled or perturbed to make the data appear as if they are numerical, and not proportional. Thus, these transformations deliver data with the same properties as shown in Figure \ref{fig:transformations}, except that the scale of the y axis is altered.

A related transformation is `rarefaction' or subsampling without replacement to a defined per-sample read count. This transformation was widely used in the 16S rRNA gene sequencing field. Rarefaction to a common read count gives a composition, that is scaled such that low count features often are replaced by 0 values [@McMurdie:2014a]. For this reason, rarefaction has now been largely replaced with the median of ratios method described below.

### The median of ratios count normalization

Further work found that none of these methods were appropriate, since the read count per sample continued to confound the analyses [@Loven:2012aa]. In other words, the TMM, RPKM, TPM methods \emph{are not scale invariant}.

Thus, the scaling normalization methods were proposed [@White:2009;@Robinson:2010a], reviewed in [@Dillies:2013]. There are several scaling normalizations, but all operate on the common assumption that by normalizing all counts in a sample to a per-sample midpoint value the normalization can impute, or at least approximate, the \emph{number} of each feature in the underlying environment.  The approaches differ largely in how the midpoint is determined. The median of ratios method called the Relative Log Expression (RLE) is instantiated in DESeq2 (and others), the trimmed mean of M values (TMM) method is used by edgeR (and others), and the Cumulative Sum Scaling (CSS) method is used by metaGenomeSeq [@White:2009] among others. The RLE method will be demonstrated and used, but the TMM and CSS methods give substantially similar results, and use the same basic logic since sample values are linearly scaled by a per-sample feature-wise midpoint; the differences are largely how those midpoints are chosen.

The RLE method calculates the ratio of the features to the geometric mean, $\mathrm{g}\vect{f}_{i-}$, of each feature across all samples, and then takes as the normalization factor the median ratio per sample as the scaling factor. Each feature is then divided by the scaling factor to place each sample on an 'equivalent' count scale. The idea is that the RLE normalization 'opens' the data from being compositional to being scaled counts. It is impossible to open the data, and while the scaled counts may have some useful properties, we see below that removing  compositional constraints are not among them.

The multi-step normalization RLE normalization attempts to normalize for sequencing depth thus 'opening' the data, and proceeeds as in the multistep Eq. \ref{eq:RLE}. Here we start with two sample vectors $\vec{\textbf{s}}_1$ and $\vec{\textbf{s}}_2$, and calculate a vector of geometric means of the features $\vec{\textbf{g}}$. Ratio vectors, $\vec{\textbf{r}}_j$ are calculated by dividing the sample vectors by the geometric mean vector, and the median of the ratio vectors is determined. Finally, the sample vectors are divided by the median of the ratio vector for each sample.


\begin{equation}
	\begin{aligned}
		\vec{\textbf{g}} = &\ \mathrm{g}\vect{f}{i-}\\
		\vec{\textbf{r}}_j = &\ \vec{\textbf{s}}_j / \vec{\textbf{g}}\\
		\vec{\textbf{d}}_j = &\ \vec{\textbf{s}}_j / Md(\vec{\textbf{r}}_j)\\
	\end{aligned}
\label{eq:RLE}
\end{equation}

A sample calculation is given in Table \ref{tab:des}, and we can see that the median ratio for each sample $\vec{\textbf{r}}_j$  samples may be different in each sample, and that the particular feature that is the median may itself be different, the median feature is in boldface in the table. Thus, by construction the feature values in each sample can be scaled by different amounts in each sample.

The RLE normalization has the attractive property that it approaches the shape of the underlying count data when the dataset is relatively well behaved. In this way it is similar to the CLR tranform. We can see this in panel RLE in Figure \ref{fig:transformations}. Here, only the extreme samples at points 15-20 diverge strongly from the values observed in the underlying count data. The RLE normalization is now widely used in both the 16S rRNA gene sequencing field [REF] and in the RNA-seq field [REF]. However, we can see that the normalization fails at the margin without warning. Thus, we can never be sure if we are comparing values correctly. An additional issue is that the RLE normalization is not compositionally appropriate, and even though it (nearly) recapitulates the overall shape of the data, it does not recapitulate the \emph{relationships} between the data across samples. Nevertheless, the RLE transformation may be somewhat useful in ideal, or nearly ideal datasets if interpreted carefully.

\begin{table}[!h]
\caption{Example calculation of RLE normalization}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c r r r r r r r}
\hline
Feature & $\vec{\textbf{s}}_1$ & $\vec{\textbf{s}}_2$ & $\vec{\textbf{g}}$ & $\vec{\textbf{r}}_1$ & $\vec{\textbf{r}}_2$ & $\vec{\textbf{d}}_1$ & $\vec{\textbf{d}}_2$ \\ \hline \hline
F1 & 1500 & 1000 & 1224.7 & 1.22 & {\bf 0.81} & 1219.5 & 1234.6\\
F2 & 25 & 15 & 19.4 & 1.29 & 0.77 & 20.3 & 18.5 \\
F3 & 1000 & 500 & 707.1 & 1.41 & 0.71 & 813.0 & 617.3 \\
F4 & 75 & 50 & 61.2 & {\bf 1.23} &  0.82 & 61.0 & 61.7 \\
F5 & 500 & 1500 & 866.0 & 0.58 & 1.73 & 406.5 & 1851.9\\ \hline
\end{tabular}
}
\label{tab:des}
\end{table}

What we see in this example is that the basis of the comparison (the denominator chosen) can be different for each sample. Thus, while there is some similarity to the CLR tranform, the RLE is expected to be generally less stable because the CLR uses the geometric mean of a basket of features to determine the basis. Thus, it is worth pointing out that the RLE normalization (and the TMM and CSS normalizations) can substantially change our interpretation of the data. As one example, the RLE  and other normaliztions obliterate our ability to determine confidence intervals for our estimates.

\begin{table}[!h]
\caption{Margin of error with RLE normalizations}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c r r r r }
\hline
Count & data size & RLE? & size & MOE \\ \hline \hline
400 & 2000 & No & 2000 & `r moe(0.2, 2000)[3]`\\
200 & 1000 & No & 1000 & `r moe(0.2, 1000)[3]`\\
50 & 250 & No & 250 & `r moe(0.2, 250)[3]`\\
20 & 100 & No & 100 & `r moe(0.2, 100)[3]`\\
400 & 2000 & Yes & 472.8 & `r moe(0.2, 472.8)[3]`\\
200 & 1000 & Yes & 472.8 & `r moe(0.2, 472.8)[3]`\\
50 & 250 & Yes & 472.8 & `r moe(0.2, 472.8)[3]`\\
20 & 100 & Yes & 472.8 & `r moe(0.2, 472.8)[3]`\\ \hline
\end{tabular}
}
\label{tab:des}
\end{table}


```{r block-malpractice, eval=F, echo=F}

###########
# variation in probabilistic sampling

# so lets look at our margin of error for a poll where the split is 20% for, 80% anti
# moe for a poll of 1000
moe(0.2, 1000)
# 0.025 so 95% CI for data are betwen 0.175 and 0.225
moe(0.2, 100)
# 0.078 CI 0.122 - 0.278

# now lets say that there are 2 different reasons that someone could be anti, but only one reason to be pro
# does this change the moe on pro? no

# lets do 4 polls of different sizes and of course answers
moe(0.25,250)
# 0.054 0.196-0.304
moe(0.19, 2000)
# 0.017 CI 0.173-0.207

# DESeq normalization
abcd <- matrix(data=c(200,800,20,80,62.5,187.5,380,1620), nrow=2, ncol=4, byrow=F)
apply(abcd,2, function(x) des.norm(x))
apply(des.norm(abcd), 2, function(x) moe(x/sum(x), sum(x)) )

col=1
abcd[1,col]/sum(abcd[,col]) - quantile(rdirichlet(100000, abcd[,col])[,1], probs=c(0.025,0.0975))
```
## Log-ratio transformations
Aitchison [-@Aitchison:1986] introduced the concept of the log-ratio transformation.


There are three main log-ratio transformations; the additive log-ratio (alr), centred log-ratio (clr) and the isometric log-ratio (ilr) [@Aitchison:1986;@pawlowsky2015modeling].

Using the same notation as above for a sample vector  $\vec{\textbf{s}}$ of $D$ `counted' features (taxa, operational taxonomic units or features, genes, etc.) $\vec{\textbf{s}}=[s_1, s_2, ... s_D]$:

The alr is the simply the elements of the sample vector divided by a presumed invariant feature, which by convention here is the last one:

\begin{equation}
\begin{aligned}
 \vec{\textbf{x}}_{alr}= &\ [log(x_1/x_D), log(x_2/x_D), \\
 & \ldots log(x_D-1/x_D]
\end{aligned}
 \label{eq:alr}
\end{equation}


This is similar to the concept used in quantitative PCR, where the relative abundance of the feature of interest is divided by the relative abundance of a (presumed) constant `housekeeping' feature. Of course there are two major drawbacks. First, that the experimentalist's knowledge of which, if any, features are invariant is necessarily incomplete. Second, is that the choice of the (presumed) invariant feature has a large effect on the result if the presumed invariant feature is not invariant, or if it is correlated with any other features in the dataset. Interestingly, an early proposal was to use the geometric mean of a number of internal controls [@Vandesompele:2002aa], leading to the next transformation.

The ALR is similar to the RLE except that the basis is chosen beforehand and may be based on a-priori information.

### The centered log-ratio transformation.

The clr is performed by taking the logarithm of the the ratio between the count value for each part and the geometric mean count: i.e., for D features in sample vector $\vect{s} = [s_1, s_2, s_3, \ldots s_D]$:

\begin{equation}
 \vect{s}_{clr}  = [log(\frac{s_1}{g\vect{s}}), log(\frac{s_2}{g\vect{s}}) \ldots log(\frac{s_D}{g\vect{s}})]
\end{equation}

where $g\vect{s} = \sqrt[D]{x_1 \cdot x_2 \cdot ... \cdot x_D}$, the geometric mean of $\vec{\textbf{x}}$.

The clr transformation is formally equivalent to a matrix of all possible pairwise ratios, but is a more tractable form. Here we are using a basket of features for the basis, not a single feature.

The clr transform is scale invariant because the same clr values are obtained from the raw counts and from the table of counts after conversion to proportions. The clr transform is sub-compositionally dominant [@pawlowsky2015modeling]. Thus, the clr transform fulfils the basic requirements of compositional data analysis.

The clr is often criticized since it has the property that the sum of the clr vector must equal 0. This constraint causes a singular covariance matrix; i.e., the sum of the covariance matrix is always a constant [@pawlowsky2015modeling]. However the clr has the advantage of being readily interpretable, a value in the vector is its abundance \emph{relative} to a mean value.

The ilr is the final transformation, and is a series of sequential log-ratios between two groups of features. For example, the philr transformation is the series of ratios between features partitioned along the phylogenetic tree [@Silverman:2017aa], although any other sequential binary partitioning  scheme is also possible [@pawlowsky2015modeling]. The ilr transformation does not suffer the drawbacks of either the alr or clr, but does not allow for insights into relationships between single features in the dataset.  Nevertheless, ilr transformations permit the full-range of multivariate tools to be used, and are recommended whenever possible.

The ilr and clr are directly comparable in a two important ways: First, the distances between samples computed using an ilr and clr transformation are equivalent. Second, the clr approaches the ilr in other respects as the number of features becomes large. In this respect, the large number of features---hundreds in the case of features, thousands in the case of genes---in a typical experiment works in our favour. Thus, while not perfect, the clr is the most widely used transformation. However, care must be taken when interpreting its outputs since single features must always be interpreted as a ratio between the feature and the denominator used for the clr transformation. The problems of using clr are apparent  when some subcomposition or group of taxa is analysed for further insight since the geometric mean of the subcomposition is not necessarily equal to that of the original composition, leading to potential inconsistencies.

Log-ratio values of any type do not need to be further normalized since the total sum is a term in both the numerator and the denominator. Likewise the clr value computed from RLE transformed data will be identical to the clr value computed from the raw counts because the RLE does not change the relationships between the features; with the exception of 0 count features. Thus, the same log-ratio value will be obtained for the vector of raw read counts, or the vector of normalized read counts, or the vector of proportions calculated from the counts. Thus, log-ratios are said to be equivalence classes such that there is no information in the total count (aside from precision) [@barcelo:2001].

Attempts to `open' the data, such as with the RLE transformation,  are doomed to failure because the data cannot be moved from the simplex to Euclidian space. The total count delivered by the sequencing instrument is a function of the instrument and not the number of molecules sampled from the environment, thus the total count has no geometric meaning. If the data are collected in such a way that the total count represents the actual count in the environment, then the data are not compositional and issues regarding compositional data disappear. However, at present all sequencing platforms deliver a fixed-sum, random sample of the proportion of molecules in the environment. Note that this does not mean that the read depth is irrelevant since more reads for a sample translate into greater precision when estimating the proportions [@fernandes:2013].

## Comparison of transformations

### A benchmark random dataset

I now set up an even simpler random dataset, composed of only four features (T, L, R, A) and 50 random samples with mean values of 100 tigers, 10000 ladybugs, 1000 Rabbits and 5 space aliens drawn from a Normal distibution---although a random uniform distribution or any other distribution will give the qualitatively the same results. I am not attempting to mimic a distribution found in a real dataset, but instead desire to show the general properties of the transformations with a simple to understand dataset. I use the dataset to show how the most common transforms compare when calculated on simulated counts, on proportions (i.e. as relative abundances after sequencing
), or RLE or CLR transformed data


```{r R-block-TLRA, results="show", echo=TRUE, message=F, error=F, warnings=F}
set.seed(13)
T <- rnorm(50, mean=100, sd=25)
L <- rnorm(50, mean=10000, sd=2500)
R <- rnorm(50, mean=1000, sd=250)
A <- rnorm(50, mean=5, sd=2.5)
ran.dat <- cbind(T,L,R,A)
ran.dat[ran.dat <=0 ] <- 0.1
```

The first row in Figure \ref{fig:r-random} shows the relationships between three features in the benchmark dataset as counts. We can see that the features are randomly normally distributed and uncorrelated in the scatter plots of counts. Most tools attempt to infer something about this numerical dataset using the dataset after sequencing, which we have seen does not deliver counts, but delivers a fixed sum dataset. For a proper analysis after sequencing, the data transforms must  be linearly related in some way to this underlying count data from the environment.

```{r r-random, fig.height=16, fig.width=9.6, results="show", echo=FALSE, message=F, error=F, warnings=F,fig.cap="Scatter plots of Ladybugs (L) vs. Tigers (T), Rabbits (R) vs. Ladybugs and Aliens (A) vs. Rabbits for simulated random Normal data. Plots are shown for the actual count data, for the proportional data, and for the proportional data after the RLE or CLR transformation."}

ran.dat.prop <- t(apply(ran.dat, 1, function(x) x/sum(x)))
ran.dat.RLE <- t(des.norm(t(ran.dat.prop)))
ran.dat.clr <- t(apply(ran.dat.prop, 1, function(x) log(x) - mean(log(x))))
dist.ran.dat <- as.matrix(dist(ran.dat, method="euclidian"))
ran.dat.num.clr <-  t(apply(ran.dat, 1, function(x) log(x) - mean(log(x))))
ran.dat.RLE.clr <-  t(apply(ran.dat.RLE, 1, function(x) log(x) - mean(log(x))))

par(mfrow=c(4,3), pch=19, col=rgb(0,0,0,0.5), cex=1.2, cex=1.2, mar=c(4,4,4,0.5))

plot(ran.dat[,"T"],ran.dat[,"L"], xlab="T", ylab="L")
plot(ran.dat[,"L"],ran.dat[,"R"], xlab="L", ylab="R", main="Counts")
plot(ran.dat[,"R"],ran.dat[,"A"], xlab="R", ylab="A")

plot(ran.dat.prop[,"T"],ran.dat.prop[,"L"],
    xlab="T.p", ylab="L.p")
plot(ran.dat.prop[,"L"],ran.dat.prop[,"R"],
    xlab="L.p", ylab="R.p", main="Proportions")
plot(ran.dat.prop[,"R"],ran.dat.prop[,"A"],
    xlab="R.p", ylab="A.p")

plot(ran.dat.RLE[,"T"],ran.dat.RLE[,"L"],
    xlab="T.RLE", ylab="L.RLE")
plot(ran.dat.RLE[,"L"],ran.dat.RLE[,"R"],
    xlab="L.RLE", ylab="R.RLE", main="RLE")
plot(ran.dat.RLE[,"R"],ran.dat.RLE[,"A"],
    xlab="R.RLE", ylab="A.RLE")

plot(ran.dat.clr[,"T"],ran.dat.clr[,"L"],
    xlab="T.CLR", ylab="L.CLR")
plot(ran.dat.clr[,"L"],ran.dat.clr[,"R"],
    xlab="L.CLR", ylab="R.CLR", main="CLR")
plot(ran.dat.clr[,"R"],ran.dat.clr[,"A"],
    xlab="R.CLR", ylab="A.CLR")

```

The second row in Figure \ref{fig:r-random} shows the same pairs of features in the same data after being converted to proportions: note that this is exactly comparable to sequencing and being constrained by an arbitrary sum as discussed in Chapter 5. Here we see that the the proportional data have a radically different internal structure. The two most relatively abundant features, R and L, which are uncorrelated in the actual data are now almost perfectly negatively correlated as proportions, R.p vs L.p. This is because the proportional data are now not real numbers, but are instead are constrained by the arbitrary sum of 1: \emph{the data are now compositional data}.

Recall from Figure \ref{fig:transformations} that the RLE and CLR transformations \emph{appeared} to restore most of the underlying structure to the data. Using this simplified dataset, we can see that this was an illusion. The RLE transformation shown in row 3 of Figure \ref{fig:r-random} seems to fix the problem caused by converting the data from numbers to proportions since the points are more spread out. However, closer inspection shows that this is not the case: compare L.RLE vs T.RLE in row 3 with L vs T in row 1. It should be clear that the RLE transformation simply \emph{spreads the points out} without restoring the actual structure of the underlying count data. This is unfortunate and misleading since the stated purpose of the RLE transformation is to recover the underlying count structure of the environmental sample after sequencing [REFS]. In the absence of a solid theoretical foundation, it is difficult to say exactly how we should interpret these RLE-transformed data. The above RLE plots were generated on the proportional dataset, however only the scale of the \emph{RLE} axes would change if the RLE normalization was conducted on the original numerical data. Thus, conclusions derived from data that are RLE-normalized actually tell us little about the underlying counts from enviroment---\emph{despite the pervasive use of this, and related, transformations in the biomedical literature}.

The last row in Fig \ref{fig:r-random} shows the proportional data transformed by the clr transformation. Again, we see that the transformed data are not similar to the actual count data. Thus, conclusions made on clr-transformed data also cannot be directly related to the count values of the actual dataset. So at this point we have a conundrum: no transformation on post-sequencing data recapitulates the pre-sequencing data that we want to examine.

## The CLR transform contains relative information

We are now in a position to realize that recovering the actual counts in the environment from the post-sequencing data is impossible without additional information. Some advocate the use of a spike-in of known numbers of a molecule that can be used to normalize. However, in practice one would need to include a sufficient number of these molecules so that the random sampling error was very small, thus decreasing the sequencing depth of the molecules being measured in the experiment. In addition, practical examination of spike-in experiments indicates that there is considerable variation in the spike-in molecules that is attributed to batch effects [BARTON], making their use suspect.

Two transforms, the RLE and clr transforms come closest to recapitulating the overall shape of the univariate data as shown in Figure \ref{fig:transformations}, and so appear to be promising. However, both transformations (and all others) fail to recapitulate the multivariate nature of the data as shown in Figure \ref{fig:r-random}. There is a way forward as long as we are willing to change our point of view from absolute numbers to relative values.

Interestingly, both the RLE transform and the clr transform are based on ratios. These tranforms share the insight that we need to examine the abundance of a feature relative to the abundance of some other feature or group of features. The RLE transform uses as the reference a median value calculated as in Equation \ref{eq:RLE}. Two aspects of this normalization are troubling. First, that the midpoint feature chosen as the reference is likely different for each sample. Second, that the values are scaled and interpreted as counts, even though the transformed values are now ratios. Third, that the transformed values will be different if the features have a different scale; i.e., if the features are multiplied by a constant.

In contrast, the clr tranform has a firm theoretical foundation based on compositional data analysis [@Aitchison:1986]. Data transformed by the clr are the same whether they are counts, or proportions or are multiplied by an arbitrary constant: they are what is called 'scale invariant'. We can demonstrate that the clr tranform provides the same relative information on the actual count data by transforming the count data, or the proportion data by the clr and plotting the result as shown in Figure \ref{fig:r-ratios}. Further, clr-transformed data are explicitly interpreted as the ratio between the count (or proportion) of a feature and the geometric mean count (or proportion) of all features. We can further modify the clr tranformation to use only those features that have particular properties (such as non-0, low variance, etc) in all samples [JIA] to avoid including features with a count of 0 in the denominator. However, in this case we must keep in mind to interpret the transformed values as the ratio between the feature and the denominator.

```{r r-ratios, fig.height=12, fig.width=15, results="show", echo=FALSE, message=F, error=F, warnings=F,fig.cap="\\label{R_random} Plot of Ladybugs vs. Tigers, Rabbits vs. Ladybugs and Aliens vs. Rabbits for simulated random Normal data after the RLE normalization (top row). Plot of input numerical and RLE normalized data for Tigers, Ladybugs and Rabbits (bottom row)"}

ran.count.clr <- t(apply(ran.dat, 1, function(x) log(x) - mean(log(x))))

par(mfrow=c(2,3), pch=19, col=rgb(0,0,0,0.5),
    cex=1.2, cex.lab=1.25)
plot(ran.count.clr[,"T"],ran.count.clr[,"L"],
    xlab="T.Count.CLR", ylab="L.Count.CLR")
plot(ran.count.clr[,"L"],ran.count.clr[,"R"],
    xlab="L.Count.CLR", ylab="R.Count.CLR", main="Count CLR")
plot(ran.count.clr[,"R"],ran.count.clr[,"A"],
    xlab="R.Count.CLR", ylab="A.Count.CLR")

plot(ran.dat.clr[,"T"],ran.dat.clr[,"L"],
    xlab="T.prop.CLR", ylab="L.prop.CLR")
plot(ran.dat.clr[,"L"],ran.dat.clr[,"R"],
    xlab="L.prop.CLR", ylab="R.prop.CLR", main="Proportion CLR")
plot(ran.dat.clr[,"R"],ran.dat.clr[,"A"],
    xlab="R.prop.CLR", ylab="A.prop.CLR")
```

