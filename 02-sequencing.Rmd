# The nature of sequencing data {#sequencing}

 There are a tremendous number of high throughput sequence analysis tools in the literature. The vast majority of these are recommended for use in only one domain. Domain specific tools are found in all experimental designs and are often touted as 'optimized for' a particular design. Another, less charitable way of describing a domain-specific tool is 'over-parameterized'. It is important to make as few assumptions as possible when examining data, and to ensure that the data being analyzed and the assumptions of the analysis tools are met  [@box:1976].


\hspace{2cm}\begin{minipage}[ct]{10cm}
\parskip=5pt
\parindent=5pt

Since all models are wrong the scientist cannot obtain a "correct" one by excessive elaboration. On the contrary following William of Occam he should seek an economical description of natural phenomena. Just as the ability to devise simple but evocative models is the signature of the great scientist so overelaboration and overparameterization is often the mark of mediocrity.\footnote{George Box 1976 \emph{Science and Statistics}. J. Am. Stat. Soc. 71:791}

\end{minipage}

## A constrained random sample

All high throughput gene sequencing datasets share a common origin and it is important to understand the source of the data. In essence, an arbitrarily large number of DNA molecules is randomly sampled from an environmental population of molecules and a small, fixed number of those molecules are sequenced.

There is a frequent assertion that data generated by high throughput sequencing instruments are counts. On the surface, this makes sense because we map reads to intervals and we observe the number of counts per interval. However,  problems arise immediately upon reflection. One pervasive issue is that the results are strongly influenced by the total read count per sample, and this is the primary reason for almost all tools and workflows using 'count normalization' methods that are introduced later.

The 'Open Random Sample' panel in Figure \ref{RALT} shows how random sampling by sequencing is usually thought of. We start with an imaginary population containing four different randomly distributed entities, tigers, ladybugs, aliens and rabbits. We want to infer the abundance of each entity in the population by taking a random sample. This is done by choosing a particular area (or volume) to sample and counting the number of each entity in the area. Doing this, we observe 14 rabbits, 1 alien, 24 ladybugs and 6 tigers. We can use this random sample to infer something about the abundances of the entities in the entire population if we know the size of the sampled area and the total size of the environmental area.

![\label{RALT} DNA sequencing is a defined-limit random sample of the environment. We start with a  population of four entities randomly distributed; rabbits, aliens, ladybugs and tigers. The standard approach is to randomly sample a fixed area of the field. This is an 'open' random sample since the \emph{number} of entities found inside the sampling area is a direct readout of the sampled area. Further, the number of one entity in the area is generally not correlated with the number of the other entities.  In contrast, DNA sequencing is constrained by the number of fragments that the machine can accomodate. This is akin to filling the squares on a checker board, where only one entity is allowed per square. This is shown in the 'closed' random sample example.](figs/RALT.pdf)

A single random sample will of course limit the precision of estimation of the population characteristics. The reason that we replicate experimental measurements is to estimate and to place limits on the precision of the estimates of population parameters. A second random sample is shown on the right and is labeled as a 'closed' random sample (the constraint will be explained shortly). The second random sample, with the same area contains 12 rabbits, one alien, 26 ladybugs and 4 tigers. The first estimate had a total of 45 entities, and the second had a total of 43 entities. Two important points need to be made. First, we are comparing the numbers of each type of entity, and second, the total number of entities identified in each sample is free to vary. In theory, we could sample the same area that is very densely populated by ladybugs (perhaps it is mating season) and identify thousands of ladybugs in a very small area.

DNA sequencing is not a counting exercise because the total number of entities found is constrained by the capacity of the instrument. Imagine the worlds worst high throughput sequencer, the GS16™ (GregSeq16™). The GS16™ has a total capacity of 16 molecules, any more and it fails, any fewer and the customer is upset because they did not receive the number of reads promised by the very slick sales brochure. The process of filling the 16 slots proceeds by a random process whereby the first entity to occupy a slot is sequenced, and any subsequent entities are unable to bind to the proprietary surface.This is akin to the processes involved in all current high throughput sequencing instruments. Applying this random process to the entities in the second sample, we observe that the GS16™ delivers 6 rabbits, 0 aliens, 8 ladybugs and 4 tigers. In the event that we sample from the area where the ladybugs were mating, we would recognize only that there were 16 ladybugs in the area, and miss entirely that the area was a seething mass of ladybugs: that is, we would be unable to distinguish if the sample just happened to be from a site that was missing the other entities, or from a site that contained a vast number of ladybugs.

## Constrained and un-constrained environments.

If we were sampling from a closed system, the closed random sample approach might be an acceptable approximation of the underlying environment. However, I argue that most environments are not closed but open, and these will be referred to as constrained and unconstrained. A constrained environment is operating at some pre-determined limit or carrying capacity, and in constrast in an unconstrained environment the total number of entities in the environment is free to change.

In Figure \ref{RALT} we were modelling what happens when the environment is un-constrained, that is, the environment can contain arbitrarily greater or fewer numbers of any entity. Two real-world examples spring to mind of such an un-constrained environment. The first example would be the consortium of organisms that inhabit the human vagina and which occur in at least two major states. The first state is dominated by one or a small number of species from the genus \emph{Lactobacillus}. The second state is dominated by a mixed population of anaerobic species. The total numbers of bacteria (bacterial load) in the second state is about 100-fold greater than in the second state [@Zozaya:2010]. When comparing either DNA or RNA molecules from these two populations we would need to ensure that we account for the un-constrained nature of the system and not assume that the total number of molecules sampled is equivalent. The second example of an un-constrained environment would be in-vitro selection experiments as published in [@mcmurrough:2014;@Wolfs:2016aa] where there are two different classes of genes; inactive genes where the absolute abundance of the corresponding DNA molecules remains unchanged over time, and active genes where the absolute abundance of the corresponding DNA molecules increases exponentially. The third example occurs when comparing normal cells and cells transformed to a cancerous phentoype by expression of an activated cMyc protein. In the cancer case, the total number of RNA molecules is several fold higher than in the matched normal tissue control cells [@Mortazavi:2008]. In all three of these types of experiment, the absolute number of molecules is free to change.

A constrained environment arises when the underlying system is operating at some fixed limit. As an example, a culture of \emph{Escherichia coli}, a common lab strain of bacteria, that is growing in the lab under controlled conditions has a near constant number of RNA molecules in each cell when grown to a predetermined cell density [@mRNA:2002]. Moreover, when a new gene is induced to make new mRNA molecules under these standard conditions the number of RNA molecules of very highly expressed genes is reduced to compensate [@Taniguchi:2010aa]. Thus, under a given condition, the total number of mRNA molecules in a cell has a fixed limit: the cell cannot exceed this limit under the given condition. In this case all mRNA molecules in the system are inherently coupled, a change in absolute abundance of one molecule is compensated for by a change in absolute abundance by one or more others to ensure that the total number of molecules in the system is relatively constant. In the language of sports analogies "a single cell cannot give 110% effort".  It is assumed that mRNA molecular abundance in most lab cultures of most cell types behave similarly.

Obviously many real datasets exist on a continuum between these example extremes, but it is not always obvious at which extreme a given sample lay. For example, when comparing gene expression in liver and kidney cells do we expect the same number of underlying molecules per cell? What about comparing liver and red blood cells? Here the answer is more obvious since the red blood cells are much less metabolically active and express far fewer unique mRNA molecules than does the typical liver cell. Knowing if the samples are from a constrained or un-constrained system has important implications for analysis as outlined below. As we shall see in the practical examples, most tools work acceptably well with constrained data, but fail in unexpected ways on un-constrained data.

## Modeling constrained and un-constrained samples

A simple thought experiment should clarify the importance of knowing the difference between constrained and un-constrained systems. In this example I generate a test dataset composed of 100 features in 20 samples---modelled as a time series with 20 steps. An equally valid way of thinking about this would be if one feature had a growth advantage over the others. There are two situations. In the 'constrained' situation, one feature increase exponentially in each time step and one other feature decreases to compensate. In the 'un-constrained' situation, only one feature increases while the remainder of the features remain at a constant total abundance.

```{r shape_data}
```

```{r shape, echo=FALSE, fig.width=6.5, fig.height=4, out.width="90%", fig.align ='center', fig.cap="Constrained and un-constrained data are very different. The \'Constrained count\' column shows a synthetic dataset where the total number of counts is a constant and this is plotted as counts on top and on a log10 scale on the bottom. Here we see that the orange and black features are compensating and perfectly negatively correlated. These same data are next converted to proportions or relative abundances and plotted in the \'Constrained prop\' column. Both the count and proportional data are the same, except for the scale, when the data are constrained. The un-constrained data behave differently. The \'Un-constrained count\' and \'Un-constrained prop\' columns are distinct and the proportional data are severely distorted. The un-constrained counts are all independent, but the  exponentially increasing feature in black is negatively correlated with the constant features in the un-constrained proportion plot. Only 5 of the 100 features are shown for clarity." }

plot_c <- function(x, main, ylab, log=""){
	plot(x[,1], pch=20, type="b",  log=log, ylim=c(min(x), max(x)),
	    xlab="time point", ylab=ylab)
	title( main=main, adj=0.5)
	points(x[,2], type="b",pch=21, col="gray")
	points(x[,3], type="b",pch=22, col="orange")
	points(x[,num.one + 10], type="b", pch=23, col="blue")
	points(x[,num.one+4], type="b", pch=24, col="red")
}

par(mfrow=c(2,4), mar=c(4,4,3,1) )

#constrained counts
plot_c(m.dub, main="Constrained\ncount", ylab="raw count")
plot_c(prop.m, main="Constrained\nprop", ylab="raw proportion")

# un-constrained counts
plot_c(m.dub.u, main="Un-constrained\ncount", ylab="raw count")
plot_c(prop.m.u, main="Un-constrained\nprop", ylab="raw proportion")

#log plots
plot_c(m.dub, main="", ylab="log10 count", log="y")
plot_c(prop.m, main="", ylab="log10 proportion", log="y")
plot_c(m.dub.u, main="", ylab="log10 count", log="y")
plot_c(prop.m.u, main="", ylab="log10 proportion", log="y")

```

Of course DNA sequencing is not the same as our synthetic example. When collecting samples and sequencing them, there are many more features, and significantly more noise because of random sampling than is modelled in Figure \ref{fig:shape}. There are many sources of sampling error or even sampling bias. Nevertheless, this example serves as an instructive starting point.

### Instrument capacity

So how is sequencing a constrained operation? Each instrument  has its own specific capacity issues that must be taken into account.

The Ion Torrent and Ion Proton systems have a chip with a predetermined number of pores on the sequencing chip (10 of thousands to 10s of millions, depending on the chip) that can accept an amplified library fragment. No signal is returned from an empty pore, and the signal is rejected if a pore contains two or more different fragments. Sequencing is successful only when the pore is occupied by a single fragment from the library. This is directly analogous to the Closed Random Sample panel in Figure \ref{RALT}

The Illumina sequencing instruments attach the DNA fragments to a glass slide and then each fragment is amplified into millions of identical fragments called clusters which appear as randomly distributed spots under a microscope. Each different Illumina instrument accommodates a characteristic maximum number of clusters, and if two or more clusters  overlap they are rejected by the software. The newest Illumina instrument, the NovaSeq, has a fixed number of cells. Thus, there is a fixed number of spots that can be accomodated on the Illumina sequencing chip just as there are a fixed number of slots on the Ion platforms or the GregSeq.

Therefore, regardless of instrument, the technician must apply a precise number of DNA molecules that maximizes the number of fragments on the sequencing instrument without overloading it. It should be obvious that loading a DNA sequencer is akin to filling the squares on a checkerboard where the goal is to have as many checkers as possible, without overlapping the pieces. DNA sequencing instruments have an upper bound on the number of fragments they can sequence, and as we shall see later, any arbitrary upper bound is equivalent to a proportion. This means that high-throughput sequencing affects the shape of the data differently on constrained and un-constrained data as shown on Figure \ref{fig:shape}.

It is often assumed that the abundance of each input DNA species that is observed after sequencing reflects in some linear way a random sample of the input  molecules. This is likely to be the case if the total number of  molecules in the input sample is constrained. Such a  constraint would be met if, for example, an increase in one or more DNA species was balanced with an equivalent decrease in one or more different species. However, as we shall see, the analysis of un-constrained data will be a problem if the limited output of the sequencing platform is not accounted for.

### Example calculation of fragment number

The number of fragments after sequencing is determined by the instrument; an Ilumina MiSeq delivers $\sim 20$M fragments whereas an Illumina NextSeq delivers $\sim 400$M and an Illumina HiSeq can deliver $\sim 250$M reads per lane on each of 8 lanes. The commonly used Nextera DNA library kit is optimized to require 50 ng of DNA per sample. Thus, the number of fragments of DNA (or RNA) molecules in the underlying environment, in general,  vastly outnumbers the number of sequence fragments from which the library is made, and the number of fragments in the library  in turn outnumbers the number of fragments from which sequencing data are ultimately derived. We can do a simple back of the envelope calculation for an example metagenomics sequencing run to show this.

Assume that we have a mixture of bacterial species with a mean genome size of 4 Mb. One mole of genomes would have a mass of $2.64 \times 10^9$ grams. A typical environmental bacterial density when collecting a metagenome sample would be on the order of at least $10^7$ bacteria per ml of sample, so if we isolate the bacteria occurring in a 1 ml sample, this corresponds to $10^7$ genomes, which corresponds to about 44 ng of DNA.

If the DNA concentration after isolation is 1 ng$/ \mu$l, and one $\mu$l of DNA is taken, this corresponds to ($1\times 10^{-9} \mathrm{\ g}) / (2.64 \times 10^9\ \mathrm{g/mole})  \times (6.02 \times 10^{23} \mathrm{\ genomes/mole}) = 228,000$ genomes. The Illumina Nextera XT kit can be used to make a library with this amount of DNA. Recall that the the DNA is fragmented, typically into 500 bp or smaller sizes. Using a fragment size of 500 bp, this corresponds to approximately $1.8 \times 10^9$ DNA fragments even for a measly 1ng of input DNA.

In the scenario where a single sample is prepared and the maximum number of fragments are loaded and run, this still results in only 1% of DNA fragments in the library being sequenced on the Illumina MiSeq, and only 22% of the DNA fragmments being sequenced on the Illumina NextSeq.

An even smaller proportion of each sample is typically sequenced. DNA sequencing rarely involves a single sample, but instead samples are 'multiplexed' on the sequencing run by mixing two or more libraries together. When this occurs the samples have a unique tag, or barcode, attached so that the samples can be uniquely identified post sequencing [@Parameswaran:2007aa;@Andersson:2008;@Gloor:2010]. Barcodes can be added by ligation or by incorporation into PCR primers that are used to amplify the library.  Obviously, increasing the number of samples through multiplexing will result in an even smaller proportion of the fragments in each  sample being sequenced.

## Sequencing post processing

After sequencing fragments are grouped in some way

### Mapping

Fragments generated from metagenomic or RNA-seq experiments are aligned to reference sequences corresponding to genes, transcripts or genetic intervals (generically genes). The total number of fragments mapping to a given gene is said to be the read count for that gene and the read count for all genes in a system ranges from 0 (no fragments align to that gene) to the total number of fragments in the sample. The output from these types of experiments is a table of read counts per gene per sample. These genes can be further aggregated into functional categories.

See later chapters for example workflows.

### OTU generation

Fragments generated from tag-sequencing are often merged into operational taxonomic units (OTUs) at some predefined percent identity, or tabulated by the number of identical fragments observed (ASUs). The output from these types of experiments is a table of counts per OTU per sample.

\clearpage
