# P-values


```{r R_block_p_val, echo=FALSE, fig.height=5, fig.width=5, fig.cap="\\label{p.val} P values represent the probability of finding the observed sample results, or more extreme results, when the null hypothesis is actually true. In other words, a p-value is the probability of seeing a difference between groups when that difference is actually due to chance. The upper left panel in the figure shows the distribution of p values for a simulated experiment where there are 500 samples in group A and 500 samples in group B and 1000 variables in each sample. The upper right panel shows the Benjamini-Hochberg corrected p-values for the same experiment. The bottom left panel shows the mean p and mean BH corrected value of 10 replicates of the upper panels. "}
# generate an empty matrix containing n rows, and 10,00 columns
n <- 1000
var <- 1000
x <- matrix(data=NA, nrow=n, ncol=var)
p <- matrix(data=NA, nrow=10, ncol=var)
for(j in 1:10){
	# fill it with independent random (pseudo) numbers
	for(i in 1:n){ x[i,] <- sample(1:1000,var) }

	# un-comment this to show what happens when one variable is truly different
	# also try BH, bonferroni
	#x[1:(n/2),1:10] <-  x[1:(n/2),1:10] * 2

	# calculate p values
	p[j,] <- apply(x, 2, function(z){as.numeric(t.test(z[1:(n/2)], z[(n/2 + 1):n])[3])})
}
mean.p <- apply(p, 2, mean)
p.adj <- apply(p, 1, function(n){p.adjust(n, method="BH")})
mean.p.adj <- apply(p.adj, 1, mean)
# plot a histogram


par(mfrow=c(2,2))
 hist(p[1,], breaks=1000, main="raw p", xlab="p")
 abline(v=0.05, col="red")
 hist(p.adj, breaks=1000, xlim=c(0,1), main="BH adj")
 abline(v=0.1, col="red")
 hist(mean.p, breaks=1000, xlim=c(0,1), main="mean raw p")
 abline(v=0.1, col="red")
hist(mean.p.adj, breaks=1000, xlim=c(0,1), main="mean BH adj")
  abline(v=0.1, col="red")
# count the number that have a p value less than 0.05
# length(which(p < 0.05))
# length(which(p.adj < 0.1))
```


A p value is the likelihood of observing the result, or one more extreme, by chance alone. The commonly used cutoff of 0.05 means that we are almost certain to observe many false positive results when the samples contain many variables. Figure \ref{pval shows an example. Here, I have chosen 1000 random numbers for each of 1000 samples. Then, I split the samples, arbitrarily, into two groups. Finally, I conducted an unpaired Welch's t-test on each of the 1000 variables in the two groups of 500 samples. The upper left panel shows a histogram of the results: p$<= 0.05$ for approximately 50 of the 1000 variables. These are shown as the p values to the left of the red line. You should be very wary of anyone who shows multivariate data, and then makes conclusions from raw p values.

One commonly used approach is to correct your p values for multiple hypothesis tests using one of the many corrections. The Benjamin-Hochberg procedure is widely used, and corrects the raw p values such that the likelihood of observing a given corrected value is the adjusted value. This is called a False Discovery Rate correction. For example, if the p value is 0.001, and the BH corrected p value is 0.1, then the likelihood that the difference is observed by chance is 10\%. Thus, if you have 100 variables that have a BH value less than 0.1, you expect that 10 of them will be false positives - you just don't know which ones. The alternative is a Family Wide Error Rate correction, the most famous of which is the Bonferroni correction. In this case the value reported is the likelihood that any of the values reported are wrong. So if 100 values are reported using a FWER cutoff of 0.1, then the likelihood that any of them are wrong is 10\%.

Another approach is to determine if the p values are stable to sampling variation. This approach is shown in the lower two panels of Figure \ref{pval}. The bottom left shows the mean p values for 10 random replicates of the data in the top left panel. Here we see that the mean p value is approximately 0.5 because the expected p value for a randomly chosen comparison is 0.5. Note that the mean BH adjusted p values approach 1.
